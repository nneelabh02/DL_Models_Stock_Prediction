{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5638d14-ccc2-48df-acae-cbb8db5dfd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import yfinance as yf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score, mean_absolute_percentage_error\n",
    "import ta # Using the 'ta' library for more features\n",
    "from datetime import datetime\n",
    "\n",
    "# ===== 0. Set Random Seeds for Reproducibility =====\n",
    "# This is a critical step to ensure that the results are the same every time the code is run.\n",
    "seed_value = 42\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value) # if you are using multi-GPU.\n",
    "    # The two lines below are often needed for full reproducibility on GPUs\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ===== 1. Download Stock Data =====\n",
    "ticker = input(\"Enter stock ticker (e.g., TSLA): \").upper()\n",
    "start_date = datetime(2015, 1, 1)\n",
    "end_date = datetime(2024, 12, 31)\n",
    "\n",
    "print(f\"Downloading {ticker} data from {start_date.date()} to {end_date.date()}...\")\n",
    "data = yf.download(ticker, start=start_date, end=end_date)\n",
    "# Flatten columns if yfinance returns a MultiIndex\n",
    "if isinstance(data.columns, pd.MultiIndex):\n",
    "    data.columns = data.columns.get_level_values(0)\n",
    "data = data[['Open', 'High', 'Low', 'Close', 'Volume']].dropna()\n",
    "print(\"Data download complete.\")\n",
    "\n",
    "\n",
    "# ===== 2. Outlier Detection and Removal using Rolling Z-Score =====\n",
    "# This robust method helps prevent the model from being skewed by extreme, one-off events.\n",
    "print(\"Identifying and removing outliers...\")\n",
    "data['Daily_Return'] = data['Close'].pct_change()\n",
    "window = 252 # Approximately one year of trading days\n",
    "rolling_mean = data['Daily_Return'].rolling(window=window).mean()\n",
    "rolling_std = data['Daily_Return'].rolling(window=window).std()\n",
    "data['Z_Score'] = (data['Daily_Return'] - rolling_mean) / rolling_std\n",
    "threshold = 3.0\n",
    "outliers = data[data['Z_Score'].abs() > threshold]\n",
    "\n",
    "if not outliers.empty:\n",
    "    print(f\"Found and removed {len(outliers)} outlier(s).\")\n",
    "    data = data.drop(outliers.index)\n",
    "else:\n",
    "    print(\"No significant outliers found.\")\n",
    "\n",
    "data = data.drop(columns=['Daily_Return', 'Z_Score'])\n",
    "print(\"Outlier removal complete.\")\n",
    "\n",
    "\n",
    "# ===== 3. Feature Engineering (using 'ta' library) =====\n",
    "# Adding a rich set of technical indicators to give the model more context.\n",
    "print(\"Adding advanced technical indicators...\")\n",
    "data['sma_10'] = ta.trend.SMAIndicator(close=data['Close'], window=10).sma_indicator()\n",
    "data['ema_10'] = ta.trend.EMAIndicator(close=data['Close'], window=10).ema_indicator()\n",
    "data['rsi'] = ta.momentum.RSIIndicator(close=data['Close'], window=14).rsi()\n",
    "macd = ta.trend.MACD(close=data['Close'])\n",
    "data['macd'] = macd.macd()\n",
    "data['macd_signal'] = macd.macd_signal()\n",
    "bollinger = ta.volatility.BollingerBands(close=data['Close'])\n",
    "data['bb_mavg'] = bollinger.bollinger_mavg()\n",
    "data['bb_hband'] = bollinger.bollinger_hband()\n",
    "data['bb_lband'] = bollinger.bollinger_lband()\n",
    "\n",
    "# Add S&P 500 Market Context\n",
    "print(\"Adding market context (S&P 500)...\")\n",
    "spy_data = yf.download('SPY', start=start_date, end=end_date)\n",
    "if isinstance(spy_data.columns, pd.MultiIndex):\n",
    "    spy_data.columns = spy_data.columns.get_level_values(0)\n",
    "spy_data['SPY_Return'] = spy_data['Close'].pct_change()\n",
    "data = data.join(spy_data['SPY_Return'])\n",
    "\n",
    "# Add time-based features\n",
    "data['DayOfWeek'] = data.index.dayofweek\n",
    "data['IsMonthStart'] = data.index.is_month_start.astype(int)\n",
    "data['IsMonthEnd'] = data.index.is_month_end.astype(int)\n",
    "data = pd.get_dummies(data, columns=['DayOfWeek'], prefix='DOW')\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "print(\"Feature engineering complete.\")\n",
    "\n",
    "# Find the index of 'Close' column BEFORE scaling\n",
    "close_price_index = data.columns.get_loc('Close')\n",
    "\n",
    "\n",
    "# ===== 4. Scale Data =====\n",
    "# We use two scalers: one for all data and one specifically for the 'Close' price.\n",
    "# This makes reversing the scaling on our predictions much more accurate.\n",
    "print(\"Scaling data...\")\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "close_price_scaler = MinMaxScaler()\n",
    "close_price_scaler.fit(data[['Close']])\n",
    "print(\"Data scaling complete.\")\n",
    "\n",
    "\n",
    "# ===== 5. Sequence Creation =====\n",
    "def create_sequences(data, seq_len, close_price_idx):\n",
    "    X, y = [], []\n",
    "    for i in range(seq_len, len(data)):\n",
    "        X.append(data[i-seq_len:i])\n",
    "        y.append(data[i, close_price_idx]) # The target is the 'Close' price\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_len = 60 # Using a 60-day window to predict the next day\n",
    "X, y = create_sequences(scaled_data, seq_len, close_price_index)\n",
    "print(f\"Created {len(X)} sequences with a window size of {seq_len} days.\")\n",
    "\n",
    "\n",
    "# ===== 6. Data Splitting (70/30) =====\n",
    "n_total = len(X)\n",
    "train_size = int(0.7 * n_total)\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_test, y_test = X[train_size:], y[train_size:]\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) # Shuffle training data\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "print(f\"Data split into {len(X_train)} training samples and {len(X_test)} testing samples.\")\n",
    "\n",
    "\n",
    "# ===== 7. Model Definition: LSTM + Transformer =====\n",
    "# This is the hybrid model from your first script.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class LSTMTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim=64, lstm_hidden=64, n_heads=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        # Project input features into the model's dimension\n",
    "        self.input_proj = nn.Linear(input_dim, model_dim)\n",
    "        # LSTM layer to process sequences\n",
    "        self.lstm = nn.LSTM(model_dim, lstm_hidden, batch_first=True, bidirectional=False)\n",
    "        # Positional encoding to give the Transformer time-context\n",
    "        self.pos_enc = PositionalEncoding(lstm_hidden)\n",
    "        # Transformer Encoder to weigh the importance of different time steps\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=lstm_hidden, nhead=n_heads, batch_first=True, activation='relu')\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        # Final fully-connected layer to produce the output\n",
    "        self.fc = nn.Linear(lstm_hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        x, _ = self.lstm(x) # LSTM processes the sequence\n",
    "        x = self.pos_enc(x) # Add positional info\n",
    "        x = self.transformer(x) # Transformer finds complex relationships\n",
    "        # We take the output from the last time step for prediction\n",
    "        out = self.fc(x[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "# ===== 8. Model Training =====\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = LSTMTransformer(input_dim=X.shape[2]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) # A smaller learning rate is often better for Adam\n",
    "\n",
    "epochs = 50\n",
    "print(f\"Starting training for {epochs} epochs...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device).unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1:2d}/{epochs} | Train Loss: {avg_train_loss:.5f}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "\n",
    "# ===== 9. Evaluation on Test Set =====\n",
    "model.eval()\n",
    "preds, actuals = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        pred = model(xb).cpu().numpy()\n",
    "        preds.extend(pred.flatten())\n",
    "        actuals.extend(yb.numpy().flatten())\n",
    "\n",
    "\n",
    "# ===== 10. Inverse Scaling using the dedicated 'close_price_scaler' =====\n",
    "# This provides a more accurate representation of the real price values.\n",
    "preds_reshaped = np.array(preds).reshape(-1, 1)\n",
    "actuals_reshaped = np.array(actuals).reshape(-1, 1)\n",
    "\n",
    "pred_unscaled = close_price_scaler.inverse_transform(preds_reshaped).flatten()\n",
    "actual_unscaled = close_price_scaler.inverse_transform(actuals_reshaped).flatten()\n",
    "\n",
    "\n",
    "# ===== 11. Report Metrics & Plot Results (Expanded) =====\n",
    "print(\"\\n--- Model Evaluation on Test Data ---\")\n",
    "r2 = r2_score(actual_unscaled, pred_unscaled)\n",
    "explained_variance = explained_variance_score(actual_unscaled, pred_unscaled)\n",
    "mae = mean_absolute_error(actual_unscaled, pred_unscaled)\n",
    "rmse = np.sqrt(mean_squared_error(actual_unscaled, pred_unscaled))\n",
    "mape = mean_absolute_percentage_error(actual_unscaled, pred_unscaled) * 100\n",
    "# Symmetric MAPE is useful as it's bounded and fair if actuals or preds are zero.\n",
    "smape_numerator = np.abs(pred_unscaled - actual_unscaled)\n",
    "smape_denominator = (np.abs(actual_unscaled) + np.abs(pred_unscaled)) / 2\n",
    "smape_mask = smape_denominator != 0\n",
    "smape = np.mean(smape_numerator[smape_mask] / smape_denominator[smape_mask]) * 100\n",
    "\n",
    "print(f\"\\n--- Evaluation for {ticker} ---\")\n",
    "print(f\"  Goodness of Fit:\")\n",
    "print(f\"    R-squared (R²):               {r2:.4f}\")\n",
    "print(f\"    Explained Variance:           {explained_variance:.4f}\")\n",
    "print(f\"\\n  Average Error:\")\n",
    "print(f\"    Mean Absolute Error (MAE):    {mae:.4f} (Error in $)\")\n",
    "print(f\"    Root Mean Squared Error (RMSE): {rmse:.4f} (Error in $)\")\n",
    "print(f\"    Mean Absolute % Error (MAPE): {mape:.2f}%\")\n",
    "print(f\"    Symmetric MAPE (SMAPE):       {smape:.2f}%\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(actual_unscaled, label='Actual Close Price', color='blue', alpha=0.9)\n",
    "plt.plot(pred_unscaled, label='Predicted Close Price', color='red', linestyle='--', alpha=0.8)\n",
    "plt.title(f\"{ticker} Stock Price Prediction (LSTM-Transformer)\", fontsize=16)\n",
    "plt.xlabel(\"Time (Test Set Days)\", fontsize=12)\n",
    "plt.ylabel(\"Close Price (USD)\", fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
