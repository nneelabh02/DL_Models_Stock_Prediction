{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597894e3-f791-4025-a7de-c1c6e231e878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import yfinance as yf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, explained_variance_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import ta\n",
    "\n",
    "# === 0. Set Random Seeds for Reproducibility ===\n",
    "# This is a critical step to ensure that the results are the same every time the code is run.\n",
    "seed_value = 42\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value) # if you are using multi-GPU.\n",
    "    # The two lines below are often needed for full reproducibility on GPUs\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# === Input stock name ===\n",
    "stock_symbol = input(\"Enter stock ticker (e.g. TSLA, AAPL, MSFT): \").upper()\n",
    "start_date = '2015-01-01'\n",
    "end_date = '2024-12-31'\n",
    "\n",
    "# 1. Download stock data\n",
    "print(f\"Downloading {stock_symbol} data...\")\n",
    "data = yf.download(stock_symbol, start=start_date, end=end_date)[['Open', 'High', 'Low', 'Close', 'Volume']].dropna()\n",
    "\n",
    "# FIX: Flatten columns if yfinance returns a MultiIndex\n",
    "if isinstance(data.columns, pd.MultiIndex):\n",
    "    data.columns = data.columns.get_level_values(0)\n",
    "\n",
    "print(\"Data download complete.\")\n",
    "\n",
    "\n",
    "# === 2. Outlier Detection and Removal using Rolling Z-Score ===\n",
    "print(\"Identifying and removing outliers...\")\n",
    "data['Daily_Return'] = data['Close'].pct_change()\n",
    "window = 252\n",
    "rolling_mean = data['Daily_Return'].rolling(window=window).mean()\n",
    "rolling_std = data['Daily_Return'].rolling(window=window).std()\n",
    "data['Z_Score'] = (data['Daily_Return'] - rolling_mean) / rolling_std\n",
    "threshold = 3.0\n",
    "outliers = data[data['Z_Score'].abs() > threshold]\n",
    "\n",
    "if not outliers.empty:\n",
    "    print(f\"Found and removed {len(outliers)} outlier(s).\")\n",
    "    data = data.drop(outliers.index)\n",
    "else:\n",
    "    print(\"No significant outliers found.\")\n",
    "\n",
    "data = data.drop(columns=['Daily_Return', 'Z_Score'])\n",
    "print(\"Outlier removal complete.\")\n",
    "\n",
    "\n",
    "# === 3. Feature Engineering (on cleaned data) ===\n",
    "print(\"Adding technical indicators...\")\n",
    "data['sma_10'] = ta.trend.SMAIndicator(close=data['Close'].squeeze(), window=10).sma_indicator()\n",
    "data['ema_10'] = ta.trend.EMAIndicator(close=data['Close'].squeeze(), window=10).ema_indicator()\n",
    "data['rsi'] = ta.momentum.RSIIndicator(close=data['Close'].squeeze(), window=14).rsi()\n",
    "data['macd'] = ta.trend.MACD(close=data['Close'].squeeze()).macd()\n",
    "data['bb_bbm'] = ta.volatility.BollingerBands(close=data['Close'].squeeze()).bollinger_mavg()\n",
    "data['day_of_week'] = data.index.dayofweek\n",
    "\n",
    "# --- Add S&P 500 Market Context ---\n",
    "print(\"Adding market context (S&P 500)...\")\n",
    "spy_data = yf.download('SPY', start=start_date, end=end_date)\n",
    "# Flatten columns for SPY data as well for robustness\n",
    "if isinstance(spy_data.columns, pd.MultiIndex):\n",
    "    spy_data.columns = spy_data.columns.get_level_values(0)\n",
    "spy_data['SPY_Return'] = spy_data['Close'].pct_change()\n",
    "data = data.join(spy_data['SPY_Return'])\n",
    "# --- End of S&P 500 section ---\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "print(\"Feature engineering complete.\")\n",
    "\n",
    "# Find the index of 'Close' column BEFORE scaling the data\n",
    "close_price_index = data.columns.get_loc('Close')\n",
    "\n",
    "# 4. Normalize Data\n",
    "scaler = MinMaxScaler()\n",
    "close_price_scaler = MinMaxScaler()\n",
    "close_price_scaler.fit(data[['Close']])\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "\n",
    "# 5. Create sequences\n",
    "def create_sequences(data, window_size, close_price_idx):\n",
    "    X, y = [], []\n",
    "    for i in range(window_size, len(data)):\n",
    "        X.append(data[i-window_size:i])\n",
    "        y.append(data[i, close_price_idx])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "window_size = 60\n",
    "# Pass the pre-calculated index to the function\n",
    "X, y = create_sequences(scaled_data, window_size, close_price_index)\n",
    "print(f\"Created {len(X)} sequences with a window size of {window_size}.\")\n",
    "\n",
    "# 6. Split Data (70/30 Train/Test)\n",
    "train_size = int(0.7 * len(X))\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_test, y_test = X[train_size:], y[train_size:]\n",
    "print(f\"Data split into {len(X_train)} training samples and {len(X_test)} testing samples.\")\n",
    "\n",
    "\n",
    "# 7. Create DataLoaders\n",
    "def to_loader(x, y, shuffle_data=True):\n",
    "    dataset = TensorDataset(torch.tensor(x, dtype=torch.float32),\n",
    "                             torch.tensor(y, dtype=torch.float32))\n",
    "    return DataLoader(dataset, batch_size=32, shuffle=shuffle_data)\n",
    "\n",
    "# Shuffling the training data is a best practice for model generalization\n",
    "train_loader = to_loader(X_train, y_train, shuffle_data=True)\n",
    "test_loader = to_loader(X_test, y_test, shuffle_data=False)\n",
    "\n",
    "\n",
    "# 8. Transformer Model Definition\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class StockTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim=32, n_heads=2, num_layers=1, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, model_dim)\n",
    "        self.pos_enc = PositionalEncoding(model_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=n_heads, batch_first=True, activation='relu')\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(model_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        x = self.pos_enc(x)\n",
    "        x = self.transformer(x)\n",
    "        return self.fc(x[:, -1, :])\n",
    "\n",
    "\n",
    "# 9. Training Loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model = StockTransformer(input_dim=X.shape[2]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "epochs = 50\n",
    "print(f\"Starting training for {epochs} epochs...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for bx, by in train_loader:\n",
    "        bx, by = bx.to(device), by.to(device).unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(bx)\n",
    "        loss = criterion(output, by)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1:2d}/{epochs} | Train Loss: {avg_train_loss:.5f}\")\n",
    "print(\"Training complete.\")\n",
    "\n",
    "\n",
    "# 10. Evaluation\n",
    "print(\"Evaluating model on the test set...\")\n",
    "model.eval()\n",
    "preds, actuals = [], []\n",
    "with torch.no_grad():\n",
    "    for bx, by in test_loader:\n",
    "        bx = bx.to(device)\n",
    "        output = model(bx).cpu().numpy()\n",
    "        preds.extend(output.flatten())\n",
    "        actuals.extend(by.numpy())\n",
    "\n",
    "\n",
    "# 11. Inverse Transform for Plotting and Metrics\n",
    "preds_reshaped = np.array(preds).reshape(-1, 1)\n",
    "actuals_reshaped = np.array(actuals).reshape(-1, 1)\n",
    "scaled_preds = close_price_scaler.inverse_transform(preds_reshaped).flatten()\n",
    "scaled_actuals = close_price_scaler.inverse_transform(actuals_reshaped).flatten()\n",
    "\n",
    "\n",
    "# 12. Calculate and Report Metrics (Expanded)\n",
    "r2 = r2_score(scaled_actuals, scaled_preds)\n",
    "explained_variance = explained_variance_score(scaled_actuals, scaled_preds)\n",
    "mae = mean_absolute_error(scaled_actuals, scaled_preds)\n",
    "rmse = np.sqrt(mean_squared_error(scaled_actuals, scaled_preds))\n",
    "mape = mean_absolute_percentage_error(scaled_actuals, scaled_preds) * 100\n",
    "smape_numerator = np.abs(scaled_preds - scaled_actuals)\n",
    "smape_denominator = (np.abs(scaled_actuals) + np.abs(scaled_preds)) / 2\n",
    "smape_mask = smape_denominator != 0\n",
    "smape = np.mean(smape_numerator[smape_mask] / smape_denominator[smape_mask]) * 100\n",
    "\n",
    "print(f\"\\n--- Evaluation for {stock_symbol} ---\")\n",
    "print(f\"  Goodness of Fit:\")\n",
    "print(f\"    R-squared (R²):                 {r2:.4f}\")\n",
    "print(f\"    Explained Variance:             {explained_variance:.4f}\")\n",
    "print(f\"\\n  Average Error:\")\n",
    "print(f\"    Mean Absolute Error (MAE):      {mae:.4f} (Error in $)\")\n",
    "print(f\"    Root Mean Squared Error (RMSE): {rmse:.4f} (Error in $)\")\n",
    "print(f\"    Mean Absolute % Error (MAPE):   {mape:.2f}%\")\n",
    "print(f\"    Symmetric MAPE (SMAPE):         {smape:.2f}%\")\n",
    "\n",
    "\n",
    "# 13. Plot Results\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(scaled_actuals, label=\"Actual Close Price\", color='blue', alpha=0.9)\n",
    "plt.plot(scaled_preds, label=\"Predicted Close Price\", color='red', linestyle='--', alpha=0.8)\n",
    "plt.title(f\"{stock_symbol} Stock Price Prediction (Transformer)\", fontsize=16)\n",
    "plt.xlabel(\"Time (Test Set Days)\", fontsize=12)\n",
    "plt.ylabel(\"Close Price (USD)\", fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
